{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "forbidden-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from pyquaternion import Quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "extreme-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import truediv, mul\n",
    "import array\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import lines\n",
    "\n",
    "matplotlib.rcdefaults()\n",
    "matplotlib.rcParams['pdf.fonttype']=42\n",
    "matplotlib.rcParams['ps.fonttype']=42\n",
    "font = {\n",
    "        'weight': 'normal',\n",
    "        'size': 13,\n",
    "            }\n",
    "matplotlib.rcParams['legend.framealpha']=0.8\n",
    "matplotlib.rc('font', **font) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "distinct-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "associate-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(map_data):\n",
    "    v = []\n",
    "    area = []\n",
    "    ent = []\n",
    "    cnt = 0\n",
    "    for i in map_data.item()['time']:\n",
    "        v.append(float(i.to_sec()))\n",
    "        if(map_data.item()['area'][cnt] != 0):\n",
    "            area.append(map_data.item()['area'][cnt])\n",
    "        else:\n",
    "            area.append(0.0025)\n",
    "        ent.append(map_data.item()['total_entropy'][cnt])\n",
    "        cnt += 1\n",
    "    d = pd.DataFrame(columns=['ts','area','ent'])\n",
    "\n",
    "    d.loc[:,'ts'] = v\n",
    "    d.loc[:,'area'] = area\n",
    "    d.loc[:,'ent'] = ent\n",
    "    d = d.set_index('ts')\n",
    "    d.index = pd.to_datetime(d.index, unit='s')\n",
    "    tmp = d.resample('2S')[['area']].agg(lambda x : np.nan if x.count() == 0 else x.idxmax()).ffill()\n",
    "    df = d.loc[tmp['area']]\n",
    "    cnt = 0\n",
    "\n",
    "    while (len(df) < 150):\n",
    "        new_row = pd.DataFrame(columns=['area','ent'])\n",
    "        new_row.loc[0 + cnt / 1000.0,'area'] = 0.0025\n",
    "        new_row.loc[0 + cnt / 1000.0,'ent'] = 1\n",
    "        cnt += 1\n",
    "        df = pd.concat([new_row, df])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "varied-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(occupancy, gt):\n",
    "    oc = []\n",
    "    se = 0\n",
    "    for l in occupancy:\n",
    "        p = float(l)\n",
    "        oc.append(p)\n",
    "    oc_gt = []\n",
    "    for l in gt:\n",
    "        p = float(l)\n",
    "        oc_gt.append(p)\n",
    "    for l in range(len(oc)):\n",
    "        if(oc[l] == -1):\n",
    "            oc[l] = 50\n",
    "        if(oc_gt[l] == -1):\n",
    "            oc_gt[l] = 50\n",
    "        se += (oc[l] - oc_gt[l])**2\n",
    "    mse = se/len(oc)\n",
    "    return mse**0.5\n",
    "\n",
    "def get_rmse_known(occupancy, gt):\n",
    "    oc = []\n",
    "    se = 0\n",
    "    for l in occupancy:\n",
    "        p = float(l)\n",
    "        oc.append(p)\n",
    "    oc_gt = []\n",
    "    for l in gt:\n",
    "        p = float(l)\n",
    "        oc_gt.append(p)\n",
    "    cnt = 0.0\n",
    "    for l in range(len(oc)):\n",
    "        if (oc[l] == -1):\n",
    "            if (oc_gt[l] != -1):\n",
    "                oc[l] = 50\n",
    "                cnt += 1\n",
    "                se += (oc[l] - oc_gt[l])**2\n",
    "        else:\n",
    "            if(oc_gt == -1):\n",
    "                oc_gt[l] = 100 - oc[l]\n",
    "            cnt += 1\n",
    "            se += (oc[l] - oc_gt[l])**2\n",
    "    mse = se/cnt\n",
    "    return mse**0.5\n",
    "\n",
    "def bac_manual(occupancy, gt):\n",
    "    oc = []\n",
    "    se = 0\n",
    "    for l in occupancy:\n",
    "        p = float(l)\n",
    "        oc.append(p)\n",
    "    oc_gt = []\n",
    "    for l in gt:\n",
    "        p = float(l)\n",
    "        oc_gt.append(p)\n",
    "    tot_free = 0\n",
    "    tot_occ = 0\n",
    "    corr_free = 0\n",
    "    corr_occ = 0\n",
    "    tot_unk = 0\n",
    "    corr_unk = 0\n",
    "    for l in range(len(oc)):\n",
    "        if oc_gt[l] == 0:\n",
    "            tot_free += 1\n",
    "            if oc[l] == oc_gt[l]:\n",
    "                corr_free += 1\n",
    "        elif oc_gt[l] == 100:\n",
    "            tot_occ += 1\n",
    "            if oc[l] == oc_gt[l]:\n",
    "                corr_occ += 1\n",
    "        elif oc_gt[l] == -1:\n",
    "            tot_unk += 1\n",
    "            if oc[l] == oc_gt[l]:\n",
    "                corr_unk += 1\n",
    "    return [1/3.0*(corr_free/tot_free + corr_occ/tot_occ + corr_unk/tot_unk),corr_free/tot_free,corr_occ/tot_occ, corr_unk/tot_unk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "broke-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "#Author   : Dr. Arun B Ayyar\n",
    "#\n",
    "#Based on : Shimazaki H. and Shinomoto S., A method for selecting the bin size of a time histogram Neural Computation (2007)\n",
    "#\t   Vol. 19(6), 1503-1527\n",
    "#\n",
    "#Data     : The duration for eruptions of the Old Faithful geyser in Yellowstone National Park (in minutes) \n",
    "#\t   or normal distribuition.\n",
    "#\t   given at http://176.32.89.45/~hideaki/res/histogram.html\n",
    "#\n",
    "#Comments : Implements a faster version than using hist from matplotlib and histogram from numpy libraries\t\n",
    "#           Also implements the shifts for the bin edges\n",
    "#\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def compute_bin_size(data):\n",
    "    data_max = max(data) #lower end of data\n",
    "    data_min = min(data) #upper end of data\n",
    "    n_min = 2500   #Minimum number of bins Ideal value = 2\n",
    "    n_max = 2501  #Maximum number of bins  Ideal value =200\n",
    "    n_shift = 1     #number of shifts Ideal value = 30\n",
    "    N = np.array(range(n_min,n_max))\n",
    "    D = float(data_max-data_min)/N    #Bin width vector\n",
    "    Cs = np.zeros((len(D),n_shift)) #Cost function vector\n",
    "    #Computation of the cost function\n",
    "    for i in range(np.size(N)):\n",
    "        shift = np.linspace(0,D[i],n_shift)\n",
    "        for j in range(n_shift):\n",
    "            edges = np.linspace(data_min+shift[j]-D[i]/2,data_max+shift[j]-D[i]/2,N[i]+1) # shift the Bin edges\n",
    "            binindex = np.digitize(data,edges) #Find binindex of each data point\n",
    "            ki= np.bincount(binindex)[1:N[i]+1] #Find number of points in each bin\n",
    "            k = np.mean(ki) #Mean of event count\n",
    "            v = sum((ki-k)**2)/N[i] #Variance of event count\n",
    "            Cs[i,j]+= (2*k-v)/((D[i])**2) #The cost Function\n",
    "    C=Cs.mean(1)\n",
    "\n",
    "    #Optimal Bin Size Selection\n",
    "    loc = np.argwhere(Cs==Cs.min())[0]\n",
    "    cmin = C.min()\n",
    "    idx  = np.where(C==cmin)\n",
    "    idx = idx[0][0]\n",
    "    optD = D[idx]\n",
    "    return np.linspace(data_min+shift[loc[1]]-D[idx]/2,data_max+shift[loc[1]]-D[idx]/2,N[idx]+1,endpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "enclosed-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder, columns, kind, df, low = 0, r = 10, ate = False, saving_results = False):\n",
    "    color = ['tab:blue','tab:green','tab:red','tab:orange','tab:purple','tab:brown','tab:pink','tab:gray','tab:olive','tab:cyan','k']\n",
    "    final_folders = []\n",
    "\n",
    "    for i in columns:\n",
    "        for k in kind:\n",
    "            final_folders.append(os.path.join(folder, i+k))\n",
    "\n",
    "    cnt = 0\n",
    "    fig1, (ax1, ax2) = plt.subplots(1,2)    \n",
    "\n",
    "    fig_map, (ax_map) = plt.subplots(1)\n",
    "    fig_exp, (ax_exp) = plt.subplots(1)\n",
    "    font = {\n",
    "        'weight': 'normal',\n",
    "        'size': 18,\n",
    "            }\n",
    "    ax_exp.set_ylabel('explored %', fontdict=font)\n",
    "    ax_exp.set_xlabel('trajectory length [m]', fontdict=font)\n",
    "    ax_map.set_ylabel('explored %', fontdict=font)\n",
    "    ax_map.set_xlabel('time [s]', fontdict=font)\n",
    "\n",
    "    ax_map_split = ax_map.twinx()\n",
    "    ax_map_split.set_ylabel('trajectory length [m]', fontdict=font)\n",
    "    ax_map_split.set_ylim([0,120])\n",
    "    sec_y = ax_exp.secondary_yaxis('right')\n",
    "    sec_y.set_ylabel('normalized entropy', fontdict=font)\n",
    "\n",
    "    for i in final_folders:\n",
    "        tot_score = []\n",
    "        tot_BAC =  []\n",
    "        tot_BAC_free = []\n",
    "        tot_BAC_occ = []\n",
    "        cam_yaw_total = []\n",
    "        rob_yaw_total = []\n",
    "        if ate:\n",
    "            tot_TRPE = []\n",
    "            tot_RRPE = []\n",
    "            tot_ATE = []\n",
    "        cov_vec = []\n",
    "        tot_lc = []\n",
    "        glob_lc = []\n",
    "        loc_lc = []\n",
    "        tot_area = []\n",
    "        area_bucketed = []\n",
    "        entropy_bucketed = []\n",
    "        pose_cov_vec = []\n",
    "        final_map_rmse = []\n",
    "        bac_manually = []\n",
    "        bac_manually_unk = []\n",
    "        bac_manually_free = []\n",
    "        bac_manually_occ = []\n",
    "        path_len =[]\n",
    "        filtered_path_df = []\n",
    "        wheel_tot = []\n",
    "        for j in range(low,r):\n",
    "            current = os.path.join(i,str(j))\n",
    "            print(current)\n",
    "            if (os.path.isfile(os.path.join(current,'map_data.npy'))):\n",
    "                if(os.path.isfile(os.path.join(current,'rposes.txt'))):\n",
    "                    raw_file = open(os.path.join(current,'rposes.txt'),'r')\n",
    "                    Lines = raw_file.readlines()\n",
    "                    tot_yaw_r = 0.0\n",
    "                    first_yaw = True\n",
    "                    prev_yaw = 0\n",
    "                    current_yaw_robot = []\n",
    "                    for l in Lines:\n",
    "                        line = l.split(' ')\n",
    "                        q = Quaternion(float(line[-1]),float(line[-4]),float(line[-3]),float(line[-2]))\n",
    "                        if first_yaw:\n",
    "                            first_yaw = False\n",
    "                            or_base = q\n",
    "                            prev_yaw = (q/or_base).yaw_pitch_roll[0]\n",
    "                            current_yaw_robot.append(prev_yaw)\n",
    "                        else:\n",
    "                            anglediff = ((q/or_base).yaw_pitch_roll[0] - prev_yaw + np.pi + np.pi*2) % (np.pi*2) - np.pi\n",
    "                            tot_yaw_r += abs(anglediff)\n",
    "                            prev_yaw = (q/or_base).yaw_pitch_roll[0]\n",
    "                            current_yaw_robot.append(prev_yaw)\n",
    "\n",
    "                    first_yaw = True\n",
    "\n",
    "                    raw_file = open(os.path.join(current,'cposes.txt'),'r')\n",
    "                    Lines = raw_file.readlines()\n",
    "                    tot_yaw_c = 0\n",
    "                    count_angles = 0\n",
    "                    for l in Lines:\n",
    "                        line = l.split(' ')\n",
    "                        q = Quaternion(float(line[-1]),float(line[-4]),float(line[-3]),float(line[-2]))\n",
    "                        if first_yaw:\n",
    "                            first_yaw = False\n",
    "                            cam_base = q\n",
    "                            yaw_wrt_r = ((q/cam_base).yaw_pitch_roll[0] - current_yaw_robot[count_angles]+ np.pi + np.pi*2) % (np.pi*2) - np.pi\n",
    "                            prev_yaw = yaw_wrt_r\n",
    "                        else:\n",
    "                            yaw_wrt_r = ((q/cam_base).yaw_pitch_roll[0] - current_yaw_robot[count_angles]+ np.pi + np.pi*2) % (np.pi*2) - np.pi\n",
    "                            anglediff = (prev_yaw - yaw_wrt_r + np.pi + np.pi*2) % (np.pi*2) - np.pi\n",
    "                            tot_yaw_c += abs(anglediff)\n",
    "                            prev_yaw = yaw_wrt_r\n",
    "                        count_angles += 1\n",
    "                        \n",
    "                    cam_yaw_total.append(tot_yaw_c)\n",
    "                    rob_yaw_total.append(tot_yaw_r)\n",
    "                map_data = np.load(os.path.join(current,'map_data.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "                system_data = np.load(os.path.join(current,'feat.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "#                 wheel_data = np.load(os.path.join(current,'wheel_from_odom.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "#                 poses = open(os.path.join(current,'poses.g2o'), 'r')\n",
    "                tot_lc.append(system_data.item()['total'][-1])\n",
    "                glob_lc.append(system_data.item()['global'][-1])\n",
    "                loc_lc.append(system_data.item()['local'][-1])\n",
    "                tot_area.append(map_data.item()['area'][-1])\n",
    "#                 wheel_tot.append(wheel_data.item()['tot_wheel'][-1])\n",
    "                wheel_found = False\n",
    "                \n",
    "                resampled = resampling(map_data)\n",
    "                area_bucketed.append(resampled[-150:].loc[:,'area'])            \n",
    "                entropy_sliced = resampled[-150:].loc[:,'ent']       \n",
    "\n",
    "                a = [0.0025 for i in range(len(area_bucketed[-1]))]\n",
    "                res = list(map(truediv, area_bucketed[-1], a))\n",
    "                res2 = list(map(truediv, entropy_sliced, res))\n",
    "                entropy_bucketed.append(res2)\n",
    "\n",
    "                path_gt = open(os.path.join(current,'rtabmap_odom.txt'),'r')\n",
    "                prev = [0,0,0]\n",
    "                first = True\n",
    "                second = True\n",
    "                path_df = pd.DataFrame(columns=['time','len'])\n",
    "                lt = 0\n",
    "                init_time = 0\n",
    "                for l in path_gt:\n",
    "                    p = l.split()\n",
    "                    if first:\n",
    "                        prev = [float(p[1]), float(p[2])]\n",
    "                        path_df.loc[p[0],'time'] = float(p[0])\n",
    "                        path_df.loc[p[0],'len'] = 0\n",
    "                        init_time =float(p[0])\n",
    "                        cum_dist = 0\n",
    "                        first = False\n",
    "                    else:\n",
    "                        if second and float(p[0])-init_time > 1:\n",
    "                            init_time = float(p[0])\n",
    "                        else:\n",
    "                            second = False\n",
    "                        cum_dist += ((prev[0]-float(p[1]))**2+(prev[1]-float(p[2]))**2)**0.5\n",
    "                        path_df.loc[p[0],'time'] = float(p[0])\n",
    "                        path_df.loc[p[0],'len'] = cum_dist\n",
    "                        prev = [float(p[1]), float(p[2])]\n",
    "                        lt = float(p[0])\n",
    "#                 for wl in range(len(wheel_data.item()['time'])):\n",
    "#                     if wheel_data.item()['time'][wl].to_sec() >= init_time:\n",
    "#                         for wl_second in range(wl+1, len(wheel_data.item()['time'])):\n",
    "#                             if wheel_data.item()['time'][wl_second].to_sec() - wheel_data.item()['time'][wl].to_sec() >= 299 or wheel_data.item()['time'][wl_second].to_sec() >= lt:\n",
    "#                                 if (wheel_data.item()['time'][wl_second].to_sec() - wheel_data.item()['time'][wl].to_sec() > 300):\n",
    "#                                     wheel_tot[-1] = abs(wheel_data.item()['tot_wheel'][wl_second] - wheel_data.item()['tot_wheel'][wl])\n",
    "#                                 else:\n",
    "#                                     wheel_tot[-1] = abs(wheel_data.item()['tot_wheel'][wl_second] - wheel_data.item()['tot_wheel'][wl])\n",
    "#                                 break\n",
    "#                         print('----')\n",
    "#                         break\n",
    "\n",
    "\n",
    "                path_len.append(cum_dist)\n",
    "                path_df.index = pd.to_datetime(path_df.index, unit='s')\n",
    "                tmp_filtered_path_df = path_df.resample('2S').max().ffill()\n",
    "                norm = 0\n",
    "                while (len(tmp_filtered_path_df) < 150):\n",
    "\n",
    "                    new_row = pd.DataFrame(columns=['time','len'])\n",
    "                    new_row.loc[0,'time'] = 0 + norm/1000.0\n",
    "                    norm += 1\n",
    "                    new_row.loc[0,'len'] = 0.00001\n",
    "                    new_row = new_row.set_index('time')\n",
    "                    new_row.index = pd.to_datetime(new_row.index, unit='s')\n",
    "                    tmp_filtered_path_df = pd.concat([new_row, tmp_filtered_path_df])\n",
    "\n",
    "                filtered_path_df.append(tmp_filtered_path_df[-150:]['len'])\n",
    "\n",
    "                # collect link variances of the graph\n",
    "#                 for l in poses:\n",
    "#                     if 'EDGE' in l:\n",
    "#                         s = l.split()\n",
    "#                         m = np.array([[float(s[-6]), float(s[-5]), float(s[-4])],\n",
    "#                                       [float(s[-5]), float(s[-3]), float(s[-2])],\n",
    "#                                       [float(s[-4]), float(s[-2]), float(s[-1])]]\n",
    "#                                     )\n",
    "#                         cov = np.linalg.inv(m)\n",
    "#                         eigen = np.linalg.eig(cov)\n",
    "#                         cov_vec.append(np.exp(1/3.0 * np.sum(np.log(eigen[0]))).real)\n",
    "                        #cov_vec.append(np.linalg.det(cov))\n",
    "        if(len(path_len)>0):\n",
    "#             wheel_over_path = list(map(truediv, wheel_tot, path_len))\n",
    "#             print(wheel_over_path)\n",
    "#             ry_over_path = list(map(truediv, rob_yaw_total, path_len))\n",
    "#             cy_over_path = list(map(truediv, cam_yaw_total, path_len))\n",
    "#             df.loc['tot_lc',i.split('/')[-1]] =  'mean: ' + str(np.mean(tot_lc)) + ' ' + ' std: ' + str(np.std(tot_lc))\n",
    "            df.loc['tot_area',i.split('/')[-1]] =  'mean: ' + str(np.mean(tot_area)) + ' ' + ' std: ' + str(np.std(tot_area))\n",
    "#             df.loc['pl',i.split('/')[-1]] =  'mean: ' + str(np.mean(path_len)) + ' ' + ' std: ' + str(np.std(path_len))\n",
    "#             df.loc['tot_y_c',i.split('/')[-1]] =  'mean: ' + str(np.mean(cam_yaw_total)) + ' ' + ' std: ' + str(np.std(cam_yaw_total))\n",
    "#             df.loc['tot_y_r',i.split('/')[-1]] =  'mean: ' + str(np.mean(rob_yaw_total)) + ' ' + ' std: ' + str(np.std(rob_yaw_total))\n",
    "#             df.loc['tot_wheel',i.split('/')[-1]] =  'mean: ' + str(np.mean(wheel_tot)) + ' ' + ' std: ' + str(np.std(wheel_tot))\n",
    "#             df.loc['final_entropy_norm',i.split('/')[-1]] =  'mean: {:.2f} std: {:.2f}'.format(round(np.mean(entropy_bucketed, axis = 0)[-1],2),round(np.std(entropy_bucketed, axis = 0)[-1],2))\n",
    "#             df.loc['final_rmse',i.split('/')[-1]] =  'mean: ' + str(np.mean(final_map_rmse)) + ' ' + ' std: ' + str(np.std(final_map_rmse))\n",
    "            if ate:\n",
    "                if (saving_results):\n",
    "                    df.loc['tot_area',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(tot_area, axis = 0),3),round(np.std(tot_area, axis = 0),3))\n",
    "                    df.loc['final_entropy_norm',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(entropy_bucketed, axis = 0)[-1],3),round(np.std(entropy_bucketed, axis = 0)[-1],3))\n",
    "#                     df.loc['per_m_wheel',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(wheel_over_path),3), + round(np.std(wheel_over_path),3))\n",
    "#                     df.loc['per_m_rob',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(ry_over_path),3), + round(np.std(ry_over_path),3))\n",
    "#                     df.loc['per_m_cam',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(cy_over_path),3), + round(np.std(cy_over_path),3))\n",
    "                    df.loc['path length',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(path_len),3), + round(np.std(path_len),3))\n",
    "#                 else:\n",
    "#                     df.loc['per_m_wheel',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(wheel_over_path),3), + round(np.std(wheel_over_path),3))\n",
    "#                     df.loc['per_m_rob',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(ry_over_path),3), + round(np.std(ry_over_path),3))\n",
    "#                     df.loc['per_m_cam',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(cy_over_path),3), + round(np.std(cy_over_path),3))\n",
    "\n",
    "#                 df.loc['tot_TRPe',i.split('/')[-1]] =  'mean: ' + str(np.mean(tot_TRPE)) + ' ' + ' std: ' + str(np.std(tot_TRPE))\n",
    "#                 df.loc['tot_RRPe',i.split('/')[-1]] =  'mean: ' + str(np.mean(tot_RRPE)) + ' ' + ' std: ' + str(np.std(tot_RRPE))\n",
    "#                 df.loc['path_len',i.split('/')[-1]] =  'mean: ' + str(np.mean(path_len)) + ' ' + ' std: ' + str(np.std(path_len))\n",
    "#                 df.loc['area over path',i.split('/')[-1]] =  'mean: ' + str(np.mean(tot_area)/np.mean(path_len))\n",
    "                lc_over_path = list(map(truediv, tot_lc, path_len))\n",
    "                print(lc_over_path)\n",
    "    \n",
    "                if (saving_results):\n",
    "                    df.loc['Loops per m',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(lc_over_path),3),round(np.std(lc_over_path),3))\n",
    "                else:\n",
    "                    df.loc['Loops per m',i.split('/')[-1]] =  'mean: {:.2f} std: {:.2f}'.format(round(np.mean(lc_over_path),3),round(np.std(lc_over_path),3))\n",
    "                \n",
    "            panda = pd.DataFrame(columns=['time','norm_entropy','area','path_len','exp'])\n",
    "            cnt_exp = 0\n",
    "            for tries in range(len(entropy_bucketed)):\n",
    "                for result in range(len(entropy_bucketed[tries])):\n",
    "                    panda.loc[cnt_exp,'norm_entropy'] = entropy_bucketed[tries][result]\n",
    "                    panda.loc[cnt_exp,'time'] = result*2\n",
    "                    panda.loc[cnt_exp,'area'] = area_bucketed[tries][result]/100.0 #area_bucketed[tries][result]\n",
    "                    panda.loc[cnt_exp,'path_len'] = filtered_path_df[tries][result]\n",
    "                    cnt_exp += 1\n",
    "            panda.norm_entropy = panda.norm_entropy.astype(float)\n",
    "            panda.path_len = panda.path_len.astype(float)\n",
    "            panda.area = panda.area.astype(float)\n",
    "            \n",
    "            sns.lineplot(data=panda, x='time', ci=None, y=\"area\", ax=ax_map, color=color[cnt], label=i.split('/')[-1].replace('E1_','').replace('E2_','').replace('_S',''))\n",
    "            sns.lineplot(data=panda, x='time', ci=None, y=\"path_len\", ax=ax_map_split, color=color[cnt])\n",
    "\n",
    "            ax_exp.plot(np.mean(filtered_path_df,axis=0), np.mean(area_bucketed, axis = 0)/100.0,color=color[cnt], label=i.split('/')[-1].replace('E1_','').replace('_S',''))\n",
    "            ax_exp.plot(np.mean(filtered_path_df,axis=0), np.mean(entropy_bucketed, axis=0),color=color[cnt], zorder=101)\n",
    "            ax_exp.set_ylim([0,1])\n",
    "            sec_y.set_ylim([0,1])\n",
    "            ax_exp.legend(ncol=2,loc='lower right')\n",
    "            \n",
    "#             ax_ent.legend()\n",
    "\n",
    "            ax_map.set_ylim([0,1])\n",
    "            ax_map.legend()\n",
    "\n",
    "#             ax_area.legend()\n",
    "\n",
    "            # plot CDF\n",
    "#             edges = compute_bin_size(cov_vec)\n",
    "#             n, bins, patches = ax1.hist(cov_vec, edges, cumulative=True, density=True, histtype='step', label=i.split('/')[-1], color=color[cnt])\n",
    "#             patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "#             ax1.plot(bins[1:], n,  label=columns[0],c=color[cnt] )\n",
    "\n",
    "            cnt += 1\n",
    "    \n",
    "#     handles, labels = ax1.get_legend_handles_labels()\n",
    "#     new_handles = [lines.Line2D([], [], c=h.get_edgecolor()) for h in handles]\n",
    "#     ax1.legend(handles=new_handles, labels=labels)\n",
    "\n",
    "#     if ate:\n",
    "#         handles, labels = ax1.get_legend_handles_labels()\n",
    "#         new_handles = [lines.Line2D([], [], c=h.get_edgecolor()) for h in handles]\n",
    "#         ax1.legend(handles=new_handles, labels=labels)\n",
    "#     fig1.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "#     fig1.show()\n",
    "    \n",
    "#     fig_map.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "    fig_map.show() \n",
    "#     fig_exp.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "    fig_exp.show()        \n",
    "    \n",
    "    separator = \"-\"\n",
    "    if (saving_results):\n",
    "        fig_map.savefig('map-{}-{}-{}.eps'.format('R',separator.join(columns),separator.join(tests)),format='eps',bbox_inches='tight')\n",
    "        fig_exp.savefig('exp-{}-{}-{}.eps'.format('R',separator.join(columns),separator.join(tests)),format='eps',bbox_inches='tight')\n",
    "    \n",
    "#     fig_area.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "#     fig_area.show()        \n",
    "    \n",
    "    display(df)\n",
    "    if (saving_results):\n",
    "        print(df.to_latex(index=True, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "realistic-latvia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ps/project/irotate/PAPER-data/R/S_A/0\n",
      "/ps/project/irotate/PAPER-data/R/S_A/1\n",
      "/ps/project/irotate/PAPER-data/R/S_A/2\n",
      "/ps/project/irotate/PAPER-data/R/S_A/3\n",
      "/ps/project/irotate/PAPER-data/R/S_A/4\n",
      "/ps/project/irotate/PAPER-data/R/S_A/5\n",
      "/ps/project/irotate/PAPER-data/R/S_A/6\n",
      "/ps/project/irotate/PAPER-data/R/S_A/7\n",
      "/ps/project/irotate/PAPER-data/R/S_A/8\n",
      "/ps/project/irotate/PAPER-data/R/S_A/9\n",
      "/ps/project/irotate/PAPER-data/R/S_A/10\n",
      "/ps/project/irotate/PAPER-data/R/S_A/11\n",
      "/ps/project/irotate/PAPER-data/R/S_A/12\n",
      "/ps/project/irotate/PAPER-data/R/S_A/13\n",
      "/ps/project/irotate/PAPER-data/R/S_A/14\n",
      "/ps/project/irotate/PAPER-data/R/S_A/15\n",
      "/ps/project/irotate/PAPER-data/R/S_A/16\n",
      "/ps/project/irotate/PAPER-data/R/S_A/17\n",
      "/ps/project/irotate/PAPER-data/R/S_A/18\n",
      "/ps/project/irotate/PAPER-data/R/S_A/19\n",
      "[3.6275232724993605, 1.63766858969415, 4.345512128231224, 0.8281301877825072, 0.7026357641983398]\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/0\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/1\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/2\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/3\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/4\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/5\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/6\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/7\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/8\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/9\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/10\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/11\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/12\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/13\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/14\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/15\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/16\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/17\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/18\n",
      "/ps/project/irotate/PAPER-data/R/S_A_O/19\n",
      "[3.6688606573981453, 1.1333430663400683, 4.936620337164438, 4.632803154272652, 4.888853069063293]\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/0\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/1\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/2\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/3\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/4\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/5\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/6\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/7\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/8\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/9\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/10\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/11\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/12\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/13\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/14\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/15\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/16\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/17\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/18\n",
      "/ps/project/irotate/PAPER-data/R/S_A_DW_O/19\n",
      "[4.428907311452266, 5.028122269763685, 1.8084713926632874, 3.2975193259850077, 0.6918041019137177]\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/0\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/1\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/2\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/3\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/4\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/5\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/6\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/7\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/8\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/9\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/10\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/11\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/12\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/13\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/14\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/15\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/16\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/17\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/18\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_0/19\n",
      "[3.107721779268417, 1.622718831748227, 3.7861407385944092, 2.648701168395585, 0.8192846371380004]\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/0\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/1\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/2\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/3\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/4\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/5\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/6\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/7\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/8\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/9\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/10\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/11\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/12\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/13\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/14\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/15\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/16\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/17\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/18\n",
      "/ps/project/irotate/PAPER-data/R/S_OL_2_3/19\n",
      "[1.400192424942455, 0.6332927526053067, 0.592537899659276, 0.9305664663753856, 1.5235632109906576]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_A</th>\n",
       "      <th>S_A_O</th>\n",
       "      <th>S_A_DW_O</th>\n",
       "      <th>S_OL_0</th>\n",
       "      <th>S_OL_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tot_area</th>\n",
       "      <td>mean: $83.257$ std: $13.239$</td>\n",
       "      <td>mean: $69.993$ std: $5.205$</td>\n",
       "      <td>mean: $73.344$ std: $1.371$</td>\n",
       "      <td>mean: $76.037$ std: $5.774$</td>\n",
       "      <td>mean: $72.924$ std: $18.419$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final_entropy_norm</th>\n",
       "      <td>mean: $0.283$ std: $0.054$</td>\n",
       "      <td>mean: $0.249$ std: $0.042$</td>\n",
       "      <td>mean: $0.235$ std: $0.015$</td>\n",
       "      <td>mean: $0.319$ std: $0.101$</td>\n",
       "      <td>mean: $0.266$ std: $0.041$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path length</th>\n",
       "      <td>mean: $21.201$ std: $3.094$</td>\n",
       "      <td>mean: $26.834$ std: $4.468$</td>\n",
       "      <td>mean: $28.751$ std: $4.666$</td>\n",
       "      <td>mean: $25.865$ std: $2.123$</td>\n",
       "      <td>mean: $23.855$ std: $2.722$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loops per m</th>\n",
       "      <td>mean: $2.228$ std: $1.488$</td>\n",
       "      <td>mean: $3.852$ std: $1.434$</td>\n",
       "      <td>mean: $3.051$ std: $1.611$</td>\n",
       "      <td>mean: $2.397$ std: $1.057$</td>\n",
       "      <td>mean: $1.016$ std: $0.384$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             S_A                        S_A_O  \\\n",
       "tot_area            mean: $83.257$ std: $13.239$  mean: $69.993$ std: $5.205$   \n",
       "final_entropy_norm    mean: $0.283$ std: $0.054$   mean: $0.249$ std: $0.042$   \n",
       "path length          mean: $21.201$ std: $3.094$  mean: $26.834$ std: $4.468$   \n",
       "Loops per m           mean: $2.228$ std: $1.488$   mean: $3.852$ std: $1.434$   \n",
       "\n",
       "                                       S_A_DW_O                       S_OL_0  \\\n",
       "tot_area            mean: $73.344$ std: $1.371$  mean: $76.037$ std: $5.774$   \n",
       "final_entropy_norm   mean: $0.235$ std: $0.015$   mean: $0.319$ std: $0.101$   \n",
       "path length         mean: $28.751$ std: $4.666$  mean: $25.865$ std: $2.123$   \n",
       "Loops per m          mean: $3.051$ std: $1.611$   mean: $2.397$ std: $1.057$   \n",
       "\n",
       "                                        S_OL_2_3  \n",
       "tot_area            mean: $72.924$ std: $18.419$  \n",
       "final_entropy_norm    mean: $0.266$ std: $0.041$  \n",
       "path length          mean: $23.855$ std: $2.722$  \n",
       "Loops per m           mean: $1.016$ std: $0.384$  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "{} &                           S_A &                        S_A_O &                     S_A_DW_O &                       S_OL_0 &                      S_OL_2_3 \\\\\n",
      "\\midrule\n",
      "tot_area           &  mean: $83.257$ std: $13.239$ &  mean: $69.993$ std: $5.205$ &  mean: $73.344$ std: $1.371$ &  mean: $76.037$ std: $5.774$ &  mean: $72.924$ std: $18.419$ \\\\\n",
      "final_entropy_norm &    mean: $0.283$ std: $0.054$ &   mean: $0.249$ std: $0.042$ &   mean: $0.235$ std: $0.015$ &   mean: $0.319$ std: $0.101$ &    mean: $0.266$ std: $0.041$ \\\\\n",
      "path length        &   mean: $21.201$ std: $3.094$ &  mean: $26.834$ std: $4.468$ &  mean: $28.751$ std: $4.666$ &  mean: $25.865$ std: $2.123$ &   mean: $23.855$ std: $2.722$ \\\\\n",
      "Loops per m        &    mean: $2.228$ std: $1.488$ &   mean: $3.852$ std: $1.434$ &   mean: $3.051$ std: $1.611$ &   mean: $2.397$ std: $1.057$ &    mean: $1.016$ std: $0.384$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['S_']#,'S_O']#,'S_O','S_DW_O']#['S',]\n",
    "tests = ['A','A_O','A_DW_O','OL_0','OL_2_3']#,'OL','I_OL','OL_FIX']#,'A_S','I_NF','NI','NI_NF',\n",
    "folder = '/ps/project/irotate/PAPER-data/'\n",
    "folder_E2 = os.path.join(folder,'R')\n",
    "\n",
    "df_2_ = pd.DataFrame()\n",
    "a = get_data(folder_E2, columns, tests, df_2_,0,20, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-hello",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-organ",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
