{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import truediv, mul\n",
    "import array\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(map_data, long=False):\n",
    "    v = []\n",
    "    area = []\n",
    "    ent = []\n",
    "    cnt = 0\n",
    "    for i in map_data.item()['time']:\n",
    "        v.append(float(i.to_sec()))\n",
    "        if(map_data.item()['area'][cnt] != 0):\n",
    "            area.append(map_data.item()['area'][cnt])\n",
    "        else:\n",
    "            area.append(0.0025)\n",
    "        ent.append(map_data.item()['total_entropy'][cnt])\n",
    "        cnt += 1\n",
    "    d = pd.DataFrame(columns=['ts','area','ent'])\n",
    "\n",
    "    d.loc[:,'ts'] = v\n",
    "    d.loc[:,'area'] = area\n",
    "    d.loc[:,'ent'] = ent\n",
    "    d = d.set_index('ts')\n",
    "    d.index = pd.to_datetime(d.index, unit='s')\n",
    "    tmp = d.resample('2S')[['area']].agg(lambda x : np.nan if x.count() == 0 else x.idxmax()).ffill()\n",
    "    df = d.loc[tmp['area']]\n",
    "    cnt = 0\n",
    "    if (not long):\n",
    "        while (len(df) < 300):\n",
    "            new_row = pd.DataFrame(columns=['area','ent'])\n",
    "            new_row.loc[0 + cnt / 1000.0,'area'] = 0.0025\n",
    "            new_row.loc[0 + cnt / 1000.0,'ent'] = 1\n",
    "            cnt += 1\n",
    "            df = pd.concat([new_row, df])\n",
    "    else:\n",
    "        while (len(df) < 375):\n",
    "            new_row = pd.DataFrame(columns=['area','ent'])\n",
    "            new_row.loc[0 + cnt / 1000.0,'area'] = 0.0025\n",
    "            new_row.loc[0 + cnt / 1000.0,'ent'] = 1\n",
    "            cnt += 1\n",
    "            df = pd.concat([new_row, df])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "#Author   : Dr. Arun B Ayyar\n",
    "#\n",
    "#Based on : Shimazaki H. and Shinomoto S., A method for selecting the bin size of a time histogram Neural Computation (2007)\n",
    "#\t   Vol. 19(6), 1503-1527\n",
    "#\n",
    "#Data     : The duration for eruptions of the Old Faithful geyser in Yellowstone National Park (in minutes) \n",
    "#\t   or normal distribuition.\n",
    "#\t   given at http://176.32.89.45/~hideaki/res/histogram.html\n",
    "#\n",
    "#Comments : Implements a faster version than using hist from matplotlib and histogram from numpy libraries\t\n",
    "#           Also implements the shifts for the bin edges\n",
    "#\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def compute_bin_size(data):\n",
    "    data_max = max(data) #lower end of data\n",
    "    data_min = min(data) #upper end of data\n",
    "    n_min = 2500   #Minimum number of bins Ideal value = 2\n",
    "    n_max = 2501  #Maximum number of bins  Ideal value =200\n",
    "    n_shift = 1     #number of shifts Ideal value = 30\n",
    "    N = np.array(range(n_min,n_max))\n",
    "    D = float(data_max-data_min)/N    #Bin width vector\n",
    "    Cs = np.zeros((len(D),n_shift)) #Cost function vector\n",
    "    #Computation of the cost function\n",
    "    for i in range(np.size(N)):\n",
    "        shift = np.linspace(0,D[i],n_shift)\n",
    "        for j in range(n_shift):\n",
    "            edges = np.linspace(data_min+shift[j]-D[i]/2,data_max+shift[j]-D[i]/2,N[i]+1) # shift the Bin edges\n",
    "            binindex = np.digitize(data,edges) #Find binindex of each data point\n",
    "            ki= np.bincount(binindex)[1:N[i]+1] #Find number of points in each bin\n",
    "            k = np.mean(ki) #Mean of event count\n",
    "            v = sum((ki-k)**2)/N[i] #Variance of event count\n",
    "            Cs[i,j]+= (2*k-v)/((D[i])**2) #The cost Function\n",
    "    C=Cs.mean(1)\n",
    "\n",
    "    #Optimal Bin Size Selection\n",
    "    loc = np.argwhere(Cs==Cs.min())[0]\n",
    "    cmin = C.min()\n",
    "    idx  = np.where(C==cmin)\n",
    "    idx = idx[0][0]\n",
    "    optD = D[idx]\n",
    "    return np.linspace(data_min+shift[loc[1]]-D[idx]/2,data_max+shift[loc[1]]-D[idx]/2,N[idx]+1,endpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(occupancy, gt):\n",
    "    oc = []\n",
    "    se = 0\n",
    "    for l in occupancy:\n",
    "        p = float(l)\n",
    "        oc.append(p)\n",
    "    oc_gt = []\n",
    "    for l in gt:\n",
    "        p = float(l)\n",
    "        oc_gt.append(p)\n",
    "    for l in range(len(oc)):\n",
    "        if(oc[l] == -1):\n",
    "            oc[l] = 50\n",
    "        if(oc_gt[l] == -1):\n",
    "            oc_gt[l] = 50\n",
    "        se += (oc[l] - oc_gt[l])**2\n",
    "    mse = se/len(oc)\n",
    "    return mse**0.5\n",
    "\n",
    "def get_rmse_known(occupancy, gt):\n",
    "    oc = []\n",
    "    se = 0\n",
    "    for l in occupancy:\n",
    "        p = float(l)\n",
    "        oc.append(p)\n",
    "    oc_gt = []\n",
    "    for l in gt:\n",
    "        p = float(l)\n",
    "        oc_gt.append(p)\n",
    "    cnt = 0.0\n",
    "    for l in range(len(oc)):\n",
    "        if (oc[l] == -1):\n",
    "            if (oc_gt[l] != -1):\n",
    "                oc[l] = 50\n",
    "                cnt += 1\n",
    "                se += (oc[l] - oc_gt[l])**2\n",
    "        else:\n",
    "            if(oc_gt == -1):\n",
    "                oc_gt[l] = 100 - oc[l]\n",
    "            cnt += 1\n",
    "            se += (oc[l] - oc_gt[l])**2\n",
    "    mse = se/cnt\n",
    "    return mse**0.5\n",
    "\n",
    "def bac_manual(occupancy, gt):\n",
    "    oc = []\n",
    "    se = 0\n",
    "    for l in occupancy:\n",
    "        p = float(l)\n",
    "        oc.append(p)\n",
    "    oc_gt = []\n",
    "    for l in gt:\n",
    "        p = float(l)\n",
    "        oc_gt.append(p)\n",
    "    tot_free = 0\n",
    "    tot_occ = 0\n",
    "    corr_free = 0\n",
    "    corr_occ = 0\n",
    "    tot_unk = 0\n",
    "    corr_unk = 0\n",
    "    for l in range(len(oc)):\n",
    "        if oc_gt[l] == 0:\n",
    "            tot_free += 1\n",
    "            if oc[l] == oc_gt[l]:\n",
    "                corr_free += 1\n",
    "        elif oc_gt[l] == 100:\n",
    "            tot_occ += 1\n",
    "            if oc[l] == oc_gt[l]:\n",
    "                corr_occ += 1\n",
    "        elif oc_gt[l] == -1:\n",
    "            tot_unk += 1\n",
    "            if oc[l] == oc_gt[l]:\n",
    "                corr_unk += 1\n",
    "    return [1/3.0*(corr_free/tot_free + corr_occ/tot_occ + corr_unk/tot_unk),corr_free/tot_free,corr_occ/tot_occ, corr_unk/tot_unk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder, columns, kind, df, low = 0, r = 10, ate = False, saving_results = False):\n",
    "    color = ['tab:blue','tab:green','tab:red','tab:orange','tab:purple','tab:brown','tab:pink','tab:gray','tab:olive','tab:cyan','k']\n",
    "    final_folders = []\n",
    "    if 'E1' in folder:\n",
    "        for i in columns:\n",
    "            for k in kind:\n",
    "                final_folders.append(os.path.join(folder, i+\"_E1_\"+k))\n",
    "    else:\n",
    "        for i in columns:\n",
    "            for k in kind:\n",
    "                final_folders.append(os.path.join(folder, i+'_E2_'+k))\n",
    "\n",
    "    cnt = 0\n",
    "    \n",
    "#     fig1, (ax1, ax2) = plt.subplots(1,2)    \n",
    "    fig_map, (ax_map) = plt.subplots(1)\n",
    "    fig_exp, (ax_exp) = plt.subplots(1)\n",
    "#     fig_ent, (ax_ent) = plt.subplots(1)\n",
    "#     fig_area, (ax_area) = plt.subplots(1)\n",
    "    font = {\n",
    "        'weight': 'normal',\n",
    "        'size': 18,\n",
    "            }\n",
    "    ax_exp.set_ylabel('explored %', fontdict=font)\n",
    "    ax_exp.set_xlabel('trajectory length [m]', fontdict=font)\n",
    "    ax_map_split = ax_map.twinx()\n",
    "    ax_map_split.set_ylabel('trajectory length [m]', fontdict=font)\n",
    "    ax_map_split.set_ylim([0,120])\n",
    "\n",
    "    sec_y = ax_exp.secondary_yaxis('right')\n",
    "    sec_y.set_ylabel('normalized entropy', fontdict=font)\n",
    "    sec_y.set_ylim([0,1])\n",
    "    ax_map.set_ylabel('explored %', fontdict=font)\n",
    "    ax_map.set_xlabel('time [s]', fontdict=font)\n",
    "    ax_map.set_ylim([0,1])\n",
    "    \n",
    "    for i in final_folders:\n",
    "        tot_score = []\n",
    "        tot_BAC =  []\n",
    "        tot_BAC_free = []\n",
    "        tot_BAC_occ = []\n",
    "        if ate:\n",
    "            tot_TRPE = []\n",
    "            tot_RRPE = []\n",
    "            tot_ATE = []\n",
    "        cov_vec = []\n",
    "        tot_lc = []\n",
    "        glob_lc = []\n",
    "        loc_lc = []\n",
    "        tot_area = []\n",
    "        area_bucketed = []\n",
    "        entropy_bucketed = []\n",
    "        pose_cov_vec = []\n",
    "        final_map_rmse = []\n",
    "        bac_manually = []\n",
    "        bac_manually_unk = []\n",
    "        bac_manually_free = []\n",
    "        bac_manually_occ = []\n",
    "        path_len =[]\n",
    "        filtered_path_df = []\n",
    "        wheel_tot = []\n",
    "        for j in range(low,r):\n",
    "            current = os.path.join(i,str(j))\n",
    "#             print(current)\n",
    "            if (os.path.isfile(os.path.join(current,'map_data.npy'))):\n",
    "                map_data = np.load(os.path.join(current,'map_data.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "                system_data = np.load(os.path.join(current,'feat.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "                wheel_data = np.load(os.path.join(current,'wheels.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "                general_results = open(os.path.join(current,'general_results.txt'), 'r')\n",
    "                gt = open(os.path.join(folder,'gt_occupancy.txt'), 'r')\n",
    "                occupancy = open(os.path.join(current,'occupancy.txt'), 'r')\n",
    "                occupancy_prob = open(os.path.join(current,'occupancy.txt'), 'r')\n",
    "                poses = open(os.path.join(current,'poses.g2o'), 'r')\n",
    "                tot_lc.append(system_data.item()['total'][-1])\n",
    "                glob_lc.append(system_data.item()['global'][-1])\n",
    "                loc_lc.append(system_data.item()['local'][-1])\n",
    "                tot_area.append(map_data.item()['area'][-1])\n",
    "                wheel_tot.append(wheel_data.item()['tot_wheel'][-1])\n",
    "\n",
    "\n",
    "                final_map_rmse.append(get_rmse(occupancy,gt))\n",
    "                gt = open(os.path.join(folder,'gt_occupancy.txt'), 'r')\n",
    "                occupancy = open(os.path.join(current,'occupancy.txt'), 'r')\n",
    "                \n",
    "                gt = open(os.path.join(folder,'gt_occupancy.txt'), 'r')\n",
    "                occupancy = open(os.path.join(current,'occupancy.txt'), 'r')\n",
    "                bac_m = bac_manual(occupancy,gt)\n",
    "                bac_manually.append(bac_m[0])\n",
    "                bac_manually_unk.append(bac_m[-1])\n",
    "                bac_manually_free.append(bac_m[1])\n",
    "                bac_manually_occ.append(bac_m[2])\n",
    "                \n",
    "                # binning the entropies and the area explored wrt 2-seconds time frames\n",
    "                resampled = resampling(map_data, \"_L\" in current)\n",
    "                if (\"_L\" in current):\n",
    "                    area_bucketed.append(resampled[-375:].loc[:,'area'])            \n",
    "                    entropy_sliced = resampled[-375:].loc[:,'ent']       \n",
    "                else:\n",
    "                    area_bucketed.append(resampled[-300:].loc[:,'area'])            \n",
    "                    entropy_sliced = resampled[-300:].loc[:,'ent']       \n",
    "                a = [0.0025 for i in range(len(area_bucketed[-1]))]\n",
    "                res = list(map(truediv, area_bucketed[-1], a))\n",
    "                res2 = list(map(truediv, entropy_sliced, res))\n",
    "                print(res2[-1])\n",
    "                entropy_bucketed.append(res2)\n",
    "\n",
    "                # ATE - slam metric\n",
    "                if ate:\n",
    "                    path_gt = open(os.path.join(current,'rtabmap_odom.txt'),'r')\n",
    "                    prev = [0,0,0]\n",
    "                    first = True\n",
    "                    path_df = pd.DataFrame(columns=['time','len'])\n",
    "                    \n",
    "                    for l in path_gt:\n",
    "                        p = l.split()\n",
    "                        if first:\n",
    "                            prev = [float(p[1]), float(p[2])]\n",
    "                            path_df.loc[p[0],'time'] = float(p[0])\n",
    "                            path_df.loc[p[0],'len'] = 0\n",
    "                            cum_dist = 0\n",
    "                            first = False\n",
    "                        else:\n",
    "                            cum_dist += ((prev[0]-float(p[1]))**2+(prev[1]-float(p[2]))**2)**0.5\n",
    "                            \n",
    "                            path_df.loc[p[0],'time'] = float(p[0])\n",
    "                            path_df.loc[p[0],'len'] = cum_dist\n",
    "                            \n",
    "                            prev = [float(p[1]), float(p[2])]\n",
    "                            \n",
    "                    path_len.append(cum_dist)\n",
    "                    path_df.index = pd.to_datetime(path_df.index, unit='s')\n",
    "                    tmp_filtered_path_df = path_df.resample('2S').max().ffill()\n",
    "                    if (\"_L\" not in current):\n",
    "                        norm = 0\n",
    "                        while (len(tmp_filtered_path_df) < 300):\n",
    "\n",
    "                            new_row = pd.DataFrame(columns=['time','len'])\n",
    "                            new_row.loc[0,'time'] = 0+ norm/1000.0\n",
    "                            new_row.loc[0,'len'] = 0.00001\n",
    "                            new_row = new_row.set_index('time')\n",
    "                            new_row.index = pd.to_datetime(new_row.index, unit='s')\n",
    "                            norm += 1\n",
    "                            tmp_filtered_path_df = pd.concat([new_row, tmp_filtered_path_df])\n",
    "                        filtered_path_df.append(tmp_filtered_path_df[-300:]['len'])\n",
    "                    else:\n",
    "                        norm = 0\n",
    "                        while (len(tmp_filtered_path_df) < 375):\n",
    "\n",
    "                            new_row = pd.DataFrame(columns=['time','len'])\n",
    "                            new_row.loc[0,'time'] = 0 + norm/1000.0\n",
    "                            norm += 1\n",
    "                            new_row.loc[0,'len'] = 0.00001\n",
    "                            new_row = new_row.set_index('time')\n",
    "                            new_row.index = pd.to_datetime(new_row.index, unit='s')\n",
    "                            tmp_filtered_path_df = pd.concat([new_row, tmp_filtered_path_df])\n",
    "                            \n",
    "                        filtered_path_df.append(tmp_filtered_path_df[-375:]['len'])\n",
    "\n",
    "                    ate = open(os.path.join(current,'ate.txt'), 'r')\n",
    "                    for l in ate:\n",
    "                        if 'rmse' in l:\n",
    "                            tot_ATE.append(float(l.split()[-2]))\n",
    "                    \n",
    "                    rpe = open(os.path.join(current,'rpe.txt'), 'r')\n",
    "                    for l in rpe:\n",
    "                        if 'translational' in l and 'rmse' in l:\n",
    "                            tot_TRPE.append(float(l.split()[-2]))\n",
    "                        elif 'rotational' in l and 'rmse' in l:\n",
    "                            tot_RRPE.append(float(l.split()[-2]))\n",
    "                            \n",
    "                    odom = np.load(os.path.join(current,'odom.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "                    for k in range(len(odom.item()['time'])):\n",
    "                        if (odom.item()['time'][-1].to_sec() - odom.item()['time'][k].to_sec() <= 603):\n",
    "                            m = np.array(odom.item()['cov'][k]).reshape((6,6))\n",
    "                            eigen = np.linalg.eig(m)\n",
    "                            pose_cov_vec.append(np.exp(1/6.0 * np.sum(np.log(eigen[0]))).real)\n",
    "                # collect map scores from general_results file\n",
    "                for l in general_results:\n",
    "                    if 'map_score' in l:\n",
    "                        tot_score.append(float(l.split()[-1]))\n",
    "                    if 'BAC score' in l:\n",
    "                        tot_BAC.append(float(l.split()[-1]))\n",
    "                    if 'correct_free_ratio' in l:\n",
    "                        tot_BAC_free.append(float(l.split()[-1]))\n",
    "                    if 'correct_occ_ratio' in l:\n",
    "                        tot_BAC_occ.append(float(l.split()[-1]))\n",
    "\n",
    "                # collect link variances of the graph\n",
    "                for l in poses:\n",
    "                    if 'EDGE' in l:\n",
    "                        s = l.split()\n",
    "                        m = np.array([[float(s[-6]), float(s[-5]), float(s[-4])],\n",
    "                                      [float(s[-5]), float(s[-3]), float(s[-2])],\n",
    "                                      [float(s[-4]), float(s[-2]), float(s[-1])]]\n",
    "                                    )\n",
    "                        cov = np.linalg.inv(m)\n",
    "                        eigen = np.linalg.eig(cov)\n",
    "                        cov_vec.append(np.exp(1/3.0 * np.sum(np.log(eigen[0]))).real)\n",
    "                        #cov_vec.append(np.linalg.det(cov))\n",
    "        if(len(tot_score)>0):\n",
    "            wheel_over_path = list(map(truediv, wheel_tot, path_len))\n",
    "            if (saving_results):\n",
    "                df.loc['BAC',i.split('/')[-1]] = 'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(bac_manually),3),round(np.std(bac_manually),3))\n",
    "                df.loc['per_m_wheel',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(wheel_over_path),3), + round(np.std(wheel_over_path),3))\n",
    "            else:\n",
    "                df.loc['per_m_wheel',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(wheel_over_path),3), + round(np.std(wheel_over_path),3))\n",
    "                df.loc['BAC',i.split('/')[-1]] = 'mean: {:.3f} std: {:.3f}'.format(round(np.mean(bac_manually),3),round(np.std(bac_manually),3))\n",
    "                \n",
    "#             df.loc['tot_area',i.split('/')[-1]] =  'mean: ' + str(np.mean(tot_area)) + ' ' + ' std: ' + str(np.std(tot_area))\n",
    "            if ate:\n",
    "                if (saving_results):\n",
    "                    df.loc['final_entropy_norm',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(entropy_bucketed, axis = 0)[-1],3),round(np.std(entropy_bucketed, axis = 0)[-1],3))\n",
    "                    df.loc['ATE RMSE',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(tot_ATE),3),round(np.std(tot_ATE),3))\n",
    "                else:\n",
    "                    df.loc['ATE RMSE',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(tot_ATE),3),round(np.std(tot_ATE),3))\n",
    "                    df.loc['final_entropy_norm',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(entropy_bucketed, axis = 0)[-1],3),round(np.std(entropy_bucketed, axis = 0)[-1],3))\n",
    "                    \n",
    "#                 df.loc['tot_TRPe',i.split('/')[-1]] =  'mean: ' + str(np.mean(tot_TRPE)) + ' ' + ' std: ' + str(np.std(tot_TRPE))\n",
    "#                 df.loc['tot_RRPe',i.split('/')[-1]] =  'mean: ' + str(np.mean(tot_RRPE)) + ' ' + ' std: ' + str(np.std(tot_RRPE))\n",
    "                df.loc['path_len',i.split('/')[-1]] =  'mean: ' + str(np.mean(path_len)) + ' ' + ' std: ' + str(np.std(path_len))\n",
    "#                 df.loc['area over path',i.split('/')[-1]] =  'mean: ' + str(np.mean(tot_area)/np.mean(path_len))\n",
    "                lc_over_path = list(map(truediv, tot_lc, path_len))\n",
    "                if (saving_results):\n",
    "                    df.loc['Loops per m',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(lc_over_path),3),round(np.std(lc_over_path),3))\n",
    "                else:\n",
    "                    df.loc['Loops per m',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(lc_over_path),3),round(np.std(lc_over_path),3))\n",
    "                \n",
    "            panda = pd.DataFrame(columns=['time','norm_entropy','area','path_len','exp'])\n",
    "            cnt_exp = 0\n",
    "            for tries in range(len(entropy_bucketed)):\n",
    "                for result in range(len(entropy_bucketed[tries])):\n",
    "                    panda.loc[cnt_exp,'norm_entropy'] = entropy_bucketed[tries][result]\n",
    "                    panda.loc[cnt_exp,'time'] = result*2\n",
    "                    if ('E2' in current):\n",
    "                        panda.loc[cnt_exp,'area'] = area_bucketed[tries][result]/204.0 #area_bucketed[tries][result]\n",
    "                    else:\n",
    "                        panda.loc[cnt_exp,'area'] = area_bucketed[tries][result]/183.0 #area_bucketed[tries][result]\n",
    "                    panda.loc[cnt_exp,'path_len'] = filtered_path_df[tries][result]\n",
    "                    cnt_exp += 1\n",
    "            panda.norm_entropy = panda.norm_entropy.astype(float)\n",
    "            panda.path_len = panda.path_len.astype(float)\n",
    "            panda.area = panda.area.astype(float)\n",
    "#             sns.lineplot(data=panda, x='time', ci=None, y=\"norm_entropy\", ax=ax_ent, color=color[cnt], label=i.split('/')[-1])\n",
    "            \n",
    "            sns.lineplot(data=panda, x='time', ci=None, y=\"area\", ax=ax_map, color=color[cnt], label=i.split('/')[-1].replace('E1_','').replace('E2_',''))\n",
    "            sns.lineplot(data=panda, x='time', ci=None, y=\"path_len\", ax=ax_map_split, color=color[cnt])\n",
    "\n",
    "#             ax_ent.plot(range(len(entropy_bucketed[0])), np.mean(entropy_bucketed, axis = 0),color=color[cnt], label=i.split('/')[-1])\n",
    "#             ax_map.plot(range(len(entropy_bucketed[0])), np.mean(area_bucketed, axis = 0),color=color[cnt], label=i.split('/')[-1])\n",
    "            if ('E2' in current):\n",
    "                ax_exp.plot(np.mean(filtered_path_df,axis=0), np.mean(area_bucketed, axis = 0)/204.0,color=color[cnt], label=i.split('/')[-1].replace('E2_',''))\n",
    "            else:\n",
    "                ax_exp.plot(np.mean(filtered_path_df,axis=0), np.mean(area_bucketed, axis = 0)/183.0,color=color[cnt], label=i.split('/')[-1].replace('E1_',''))\n",
    "#             ax_area.plot(range(len(entropy_bucketed[0]))[10:], list(map(truediv,np.mean(filtered_path_df, axis = 0), np.mean(area_bucketed, axis = 0)))[10:],color=color[cnt], label=i.split('/')[-1])\n",
    "            ax_exp.plot(np.mean(filtered_path_df,axis=0), np.mean(entropy_bucketed, axis=0),color=color[cnt])\n",
    "\n",
    "            ax_exp.legend(ncol=2)\n",
    "            \n",
    "#             ax_ent.legend()\n",
    "\n",
    "            ax_map.legend(ncol=2)\n",
    "\n",
    "#             ax_area.legend()\n",
    "\n",
    "            # plot CDF\n",
    "#             edges = compute_bin_size(cov_vec)\n",
    "#             n, bins, patches = ax1.hist(cov_vec, edges, cumulative=True, density=True, histtype='step', label=i.split('/')[-1], color=color[cnt])\n",
    "#             patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "#             #ax1.plot(bins, n,  label=columns[cnt],c=np.random.rand(3,) )\n",
    "#             if ate:\n",
    "#                 edges = compute_bin_size(pose_cov_vec)\n",
    "#                 n, bins, patches = ax2.hist(pose_cov_vec, edges, cumulative=True, density=True, histtype='step', label=i.split('/')[-1], color=color[cnt])\n",
    "#                 patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "                #ax2.plot(bins[1:], n,  ,c=np.random.rand(3,) )\n",
    "            cnt += 1\n",
    "#             print(tot_area)\n",
    "            print(len(tot_lc))\n",
    "            print('---------')\n",
    "    \n",
    "#     handles, labels = ax1.get_legend_handles_labels()\n",
    "#     new_handles = [lines.Line2D([], [], c=h.get_edgecolor()) for h in handles]\n",
    "#     ax1.legend(handles=new_handles, labels=labels)\n",
    "\n",
    "#     if ate:\n",
    "#         handles, labels = ax2.get_legend_handles_labels()\n",
    "#         new_handles = [lines.Line2D([], [], c=h.get_edgecolor()) for h in handles]\n",
    "#         ax2.legend(handles=new_handles, labels=labels)\n",
    "#     fig1.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "#     fig1.show()\n",
    "    \n",
    "#     fig_map.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "    fig_map.show() \n",
    "#     fig_exp.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "    fig_exp.show()        \n",
    "    \n",
    "    separator = \"-\"\n",
    "    if (saving_results):\n",
    "        fig_map.savefig('map-{}-{}-{}.eps'.format('E1' if 'E1' in folder else 'E2',separator.join(columns),separator.join(tests)),format='eps',bbox_inches='tight')\n",
    "        fig_exp.savefig('exp-{}-{}-{}.eps'.format('E1' if 'E1' in folder else 'E2',separator.join(columns),separator.join(tests)),format='eps',bbox_inches='tight')\n",
    "    \n",
    "#     fig_area.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "#     fig_area.show()        \n",
    "    \n",
    "    display(df)\n",
    "    if (saving_results):\n",
    "        print(df.to_latex(index=True, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-logistics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcdefaults()\n",
    "matplotlib.rcParams['pdf.fonttype']=42\n",
    "matplotlib.rcParams['ps.fonttype']=42\n",
    "font = {\n",
    "        'weight': 'normal',\n",
    "        'size': 13,\n",
    "            }\n",
    "matplotlib.rc('font', **font) \n",
    "\n",
    "columns = ['S']#,'S_O']#,'S_O','S_DW_O']#['S',]\n",
    "tests = ['A','A_S','A_1','INTER_0','OL_0']#,'OL','I_OL','OL_FIX']#,'A_S','I_NF','NI','NI_NF',\n",
    "folder = '/ps/project/irotate/PAPER-data/'\n",
    "folder_E2 = os.path.join(folder,'E1')\n",
    "\n",
    "df_2_ = pd.DataFrame()\n",
    "a = get_data(folder_E2, columns, tests, df_2_,0,20, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-sensitivity",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = ['S']#,'S_O']#,'S_O','S_DW_O']#['S',]\n",
    "tests = ['A','A_S','A_1','INTER_0','OL_0']#,'OL','I_OL','OL_FIX']#,'A_S','I_NF','NI','NI_NF',\n",
    "folder = '/ps/project/irotate/PAPER-data/'\n",
    "folder_E2 = os.path.join(folder,'E2')\n",
    "\n",
    "df_2_ = pd.DataFrame()\n",
    "a = get_data(folder_E2, columns, tests, df_2_,0,20, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-taylor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = ['S']#,'S_O']#,'S_O','S_DW_O']#['S',]\n",
    "tests = ['OL_0','OL_1','OL_1_3','OL_2','OL_2_3']#,'OL','I_OL','OL_FIX']#,'A_S','I_NF','NI','NI_NF',\n",
    "folder = '/ps/project/irotate/PAPER-data/'\n",
    "folder_E2 = os.path.join(folder,'E1')\n",
    "\n",
    "df_2_ = pd.DataFrame()\n",
    "a = get_data(folder_E2, columns, tests, df_2_,0,20, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-croatia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = ['S']#,'S_O']#,'S_O','S_DW_O']#['S',]\n",
    "tests = ['OL_0','OL_1','OL_1_3','OL_2','OL_2_3']#,'OL','I_OL','OL_FIX']#,'A_S','I_NF','NI','NI_NF',\n",
    "folder = '/ps/project/irotate/PAPER-data/'\n",
    "folder_E2 = os.path.join(folder,'E2')\n",
    "\n",
    "df_2_ = pd.DataFrame()\n",
    "a = get_data(folder_E2, columns, tests, df_2_,0,20, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['S']#,'S_O']#,'S_O','S_DW_O']#['S',]\n",
    "tests = ['A','A_DW_O','A_O','OL_2_3','A_L']#,'OL','I_OL','OL_FIX']#,'A_S','I_NF','NI','NI_NF',\n",
    "folder = '/ps/project/irotate/PAPER-data/'\n",
    "folder_E2 = os.path.join(folder,'E2')\n",
    "\n",
    "df_2_ = pd.DataFrame()\n",
    "a = get_data(folder_E2, columns, tests, df_2_, 0,20, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['S']#,'S_O']#,'S_O','S_DW_O']#['S',]\n",
    "tests = ['A','A_DW_O','A_O','OL_2_3','A_L']#,'OL','I_OL','OL_FIX']#,'A_S','I_NF','NI','NI_NF',\n",
    "folder = '/ps/project/irotate/PAPER-data/'\n",
    "folder_E2 = os.path.join(folder,'E1')\n",
    "\n",
    "df_2_ = pd.DataFrame()\n",
    "a = get_data(folder_E2, columns, tests, df_2_, 0,20, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-hybrid",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
